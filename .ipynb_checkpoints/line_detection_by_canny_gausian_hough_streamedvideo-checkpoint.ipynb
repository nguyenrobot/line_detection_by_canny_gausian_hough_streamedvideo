{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line detection with Canny Filter and Hough Transform on streamed-video\n",
    "    \n",
    "In this tutorial, we apply the same technic based on Canny Filter and Hough Transform to detect lines.\n",
    "*My previous tutorial on line-detection based on Canny Filter and Hough Transform of a static image*\n",
    "https://github.com/nguyenrobot/line_detection_by_canny_gausian_hough  \n",
    "\n",
    "This time we will try to detect lane's lines of a streamed video instead of a static image.\n",
    "\n",
    "Our processing consist of (for the first part of this tutorial) :\n",
    "- [x] Colour selection, try to keep white and yellow pixels\n",
    "- [x] Gaussian filter with small kernel size to detect even blurred lines in far left/right side\n",
    "- [x] Canny edge detection with small values of threshold to detect even blurred lines in far left/right side\n",
    "- [x] Zone of interest filtering, to eliminate non-desired detections\n",
    "- [x] Probabilistic Hough Transform with small values of minLineLength, maxLineGap and minimum_vote to be able to detect dashed-lines in far left/right side\n",
    "\n",
    "*Author : nguyenrobot*  \n",
    "*Copyright : nguyenrobot*  \n",
    "https://github.com/nguyenrobot\n",
    "\n",
    "Preprocessed-video Credit : Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Greedy Strategy, we try to detect everything with small thresholds and accept false-positive as a compromise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    `vertices` should be a numpy array of integer points.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    \"\"\"\n",
    "    NOTE: this is the function you might want to use as a starting point once you want to \n",
    "    average/extrapolate the line segments you detect to map out the full\n",
    "    extent of the lane (going from the result shown in raw-lines-example.mp4\n",
    "    to that shown in P1_example.mp4).  \n",
    "    \n",
    "    Think about things like separating line segments by their \n",
    "    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n",
    "    line vs. the right line.  Then, you can average the position of each of \n",
    "    the lines and extrapolate to the top and bottom of the lane.\n",
    "    \n",
    "    This function draws `lines` with `color` and `thickness`.    \n",
    "    Lines are drawn on the image inplace (mutates the image).\n",
    "    If you want to make the lines semi-transparent, think about combining\n",
    "    this function with the weighted_img() function below\n",
    "    \"\"\"\n",
    "    frame_lines = np.copy(img)\n",
    "    frame_lines[:,:] = [0, 0, 0]\n",
    "    \n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                frame_lines[y1, x1] = [255, 255, 255]\n",
    "                frame_lines[y2, x2] = [255, 255, 255]\n",
    "                #cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "    frame_lines     = grayscale(frame_lines)\n",
    "    # Hough Transform\n",
    "    minLineLength   = 1\n",
    "    maxLineGap      = 50\n",
    "    rho             = 1\n",
    "    theta           = np.pi/1440\n",
    "    minimum_vote    = 5\n",
    "    lines_2nd_hough = hough_lines(frame_lines, rho, theta, minimum_vote, minLineLength, maxLineGap)\n",
    "    \n",
    "    if lines_2nd_hough is not None:\n",
    "        for line in lines_2nd_hough:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                cv2.line(img, (x1, y1), (x2, y2), color, thickness)    \n",
    "    \n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    #line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    #draw_lines(line_img, lines)\n",
    "    return lines# ,line_img\n",
    "\n",
    "# Python 3 has support for cool math symbols.\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + γ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, γ)\n",
    "\n",
    "def color_selection(frame, RGB_thd):\n",
    "# Define color selection threshold\n",
    "# Example : to keep white and yellow RGB_thd should be\n",
    "# RGB_thd = [[200, 200, 200], [200, 200, 0]]\n",
    "\n",
    "    color_selection_ind = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
    "    result              = np.copy(frame)\n",
    "    \n",
    "    # Define selection by color / below the threshold\n",
    "    for RGB_thd_i in RGB_thd:\n",
    "        color_selection_ind[:,:] = ((result[:,:,0] > RGB_thd_i[0]) & \\\n",
    "                                    (result[:,:,1] > RGB_thd_i[1]) & \\\n",
    "                                    (result[:,:,2] > RGB_thd_i[2])) \\\n",
    "                                    | color_selection_ind[:,:]\n",
    "    result[~color_selection_ind] = [0, 0, 0]\n",
    "    #result                     = np.copy(result[color_selection_ind])\n",
    "    return result, color_selection_ind\n",
    "\n",
    "def process_image(image):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # TODO: put your pipeline here,\n",
    "    # you should return the final output (image where lines are drawn on lanes)\n",
    "    result = image\n",
    "       \n",
    "    # Colour Selection\n",
    "    RGB_thd               = [[125, 125, 125], [150, 150, 0]]\n",
    "    #RGB_thd               = [[115, 115, 115]]\n",
    "    result, color_selection_ind = color_selection(result, RGB_thd)\n",
    "\n",
    "    # Gray scale\n",
    "    result = grayscale(result)\n",
    "    \n",
    "    # Gaussian filter\n",
    "    kernel_size     = 11\n",
    "    result = gaussian_blur(result, kernel_size)\n",
    "\n",
    "    # Canny edge detection\n",
    "    low_threshold   = 5\n",
    "    high_threshold  = 15\n",
    "    result = canny(result, low_threshold, high_threshold)\n",
    "    \n",
    "    # Zone of interest filtering\n",
    "    vertices        = np.array([[(0,np.int(image.shape[0]/2) + 100), \\\n",
    "                      (0,image.shape[0]-1), \\\n",
    "                      (image.shape[1]-1,image.shape[0]-1), \\\n",
    "                      (image.shape[1]-1 - 0,np.int(image.shape[0]/2) + 100), \\\n",
    "                      (np.int(image.shape[1]/2) + 100, np.int(image.shape[0]/2) + 50), \\\n",
    "                      (np.int(image.shape[1]/2) - 100, np.int(image.shape[0]/2) + 50)]], \\\n",
    "                    dtype=np.int32)\n",
    "    result = region_of_interest(result, vertices)\n",
    "    \n",
    "    # Hough Transform\n",
    "    minLineLength   = 1\n",
    "    maxLineGap      = 15\n",
    "    rho             = 1\n",
    "    theta           = np.pi/1440\n",
    "    minimum_vote    = 25\n",
    "    lines           = hough_lines(result, rho, theta, minimum_vote, minLineLength, maxLineGap)\n",
    "    \n",
    "    # Draw lines\n",
    "    line_img = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img, lines)\n",
    "    \n",
    "    # Display the result on original video\n",
    "    result          = weighted_img(line_img, image)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will work on road videos coming from Udacity.\n",
    "\n",
    "If our algorithm is robust, it should detect :\n",
    "*    Solid-line and dashed-line of ego-vehicle's lane\n",
    "*    Road-edge line\n",
    "*    Dashed-lines of next-lanes\n",
    "\n",
    "## Let's see how far can we go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['challenge.mp4', 'solidWhiteRight.mp4', 'solidYellowLeft.mp4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"test_videos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a basic video which contains sharp and clear white lines.\n",
    "### solidWhiteRight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed-video Credit : Udacity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|                                                                             | 0/221 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed-video Credit : Udacity\n",
      "Moviepy - Building video test_videos_output/solidWhiteRight_out.mp4.\n",
      "Moviepy - Writing video test_videos_output/solidWhiteRight_out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test_videos_output/solidWhiteRight_out.mp4\n",
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/solidWhiteRight_out.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Preprocessed-video Credit : Udacity')\n",
    "#Load video\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip1        = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "print('Preprocessed-video Credit : Udacity')\n",
    "\n",
    "#Process loaded video\n",
    "white_clip   = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "\n",
    "#Write output video\n",
    "white_output = 'test_videos_output/solidWhiteRight_out.mp4'\n",
    "%time white_clip.write_videofile(white_output, audio=False)\n",
    "\n",
    "#Display processed video\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is quite good, our algorithm can dectect lines of ego-vehicle's lane but also these next-lanes' lines.  \n",
    "But, the yellow grass on the far right side is also detected as false positive lines because we want to keep Yellow and White pixels will colour selection :  \n",
    "    # Colour Selection\n",
    "    RGB_thd               = [[125, 125, 125], [150, 150, 0]]\n",
    "    #RGB_thd               = [[115, 115, 115]]\n",
    "    result, color_selection_ind = color_selection(result, RGB_thd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a try with a harder video which contains yellow solid-line and road-edge lines.\n",
    "### solidYellowLeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessed-video Credit : Udacity')\n",
    "#Load video\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip2        = VideoFileClip(\"test_videos/solidYellowLeft.mp4\")\n",
    "\n",
    "#Process loaded video\n",
    "yellow_clip   = clip2.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "\n",
    "#Write output video\n",
    "yellow_output = 'test_videos_output/solidYellowLeft_out.mp4'\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)\n",
    "\n",
    "#Display processed video\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is really terrible :\n",
    "    * By putting small thresholds for Canny Edge Detection, we even have false positive detection of dirties on wind-shield\n",
    "    * Many false-positives on far-left side because we keep also light-yellow pixels. We did not distinguish dark-yellow (colour of the left solid-line) from light-yellow of death grasses\n",
    "    * Hough Transform with small thresholds causes many tiny false-positive all around the video frame but also helps us detect well dashed-lines on the far-right side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Greedy Strategy with enhanced colour selection\n",
    "We always want to detect dashed-lines on far right/left side where that become blurred and small in the video's frame.\n",
    "We will to keep our greedy strategy and try to make a better colour selection..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce color_keep_range and color_remove_range.\n",
    "    # Colour Selection\n",
    "    \n",
    "    # keep white and yellow pixels\n",
    "    RGB_thd_keep = [([150, 150, 150], [255, 255, 255]), ([160, 160, 100], [255, 210, 140])]\n",
    "    result, k                          = color_keep_range(result, RGB_thd_keep)\n",
    "    \n",
    "    # remove unwanted colours\n",
    "    RGB_thd_remove = [([0, 0, 0], [215, 215, 180])]\n",
    "    result, k                          = color_remove_range(result, RGB_thd_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used RGB pick to identify numerically [R G B] value of pixels that need to be removed :  color_keep_range\n",
    "RGB pick &#8595;<img src=\"images/grass_color_picked.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also put kernel_size at a higher value (15) to keep dirties on windshield out of line detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_keep_range(frame, RGB_thd):\n",
    "# Define color selection threshold - keep in-range pixels\n",
    "# Example : to keep white and yellow RGB_thd should be\n",
    "# RGB_thd = [([200, 200, 200], [255, 255, 255]), ([150, 150, 100], [180, 180, 120])]\n",
    "\n",
    "    color_selection_ind = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
    "    result              = np.copy(frame)\n",
    "    \n",
    "    # Define selection by color / below the threshold\n",
    "    for RGB_thd_i in RGB_thd:\n",
    "        color_selection_ind[:,:] = ((result[:,:,0] > RGB_thd_i[0][0]) & (result[:,:,0] < RGB_thd_i[1][0]) & \\\n",
    "                                    (result[:,:,1] > RGB_thd_i[0][1]) & (result[:,:,1] < RGB_thd_i[1][1]) & \\\n",
    "                                    (result[:,:,2] > RGB_thd_i[0][2]) & (result[:,:,2] < RGB_thd_i[1][2]))\\\n",
    "                                    | color_selection_ind[:,:]\n",
    "\n",
    "    result[~color_selection_ind] = [0, 0, 0]\n",
    "    return result, color_selection_ind\n",
    "\n",
    "def color_remove_range(frame, RGB_thd):\n",
    "# Define color selection threshold - remove in-range pixels\n",
    "# Example : to remove white and yellow RGB_thd should be\n",
    "# RGB_thd = [([200, 200, 200], [255, 255, 255]), ([150, 150, 100], [180, 180, 120])]\n",
    "\n",
    "    color_remove_ind         = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
    "    color_remove_ind_temp    = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
    "    result                   = np.copy(frame)\n",
    "    \n",
    "    # Define selection by color / below the threshold\n",
    "    for RGB_thd_i in RGB_thd:\n",
    "        color_remove_ind_temp[:,:] = ((result[:,:,0] > RGB_thd_i[0][0]) & (result[:,:,0] < RGB_thd_i[1][0]) & \\\n",
    "                                    (result[:,:,1] > RGB_thd_i[0][1]) & (result[:,:,1] < RGB_thd_i[1][1]) & \\\n",
    "                                    (result[:,:,2] > RGB_thd_i[0][2]) & (result[:,:,2] < RGB_thd_i[1][2]))\n",
    "        color_remove_ind           = color_remove_ind | color_remove_ind_temp\n",
    "\n",
    "    result[color_remove_ind] = [0, 0, 0]\n",
    "    return result, color_remove_ind\n",
    "\n",
    "def process_image(image):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # TODO: put your pipeline here,\n",
    "    # you should return the final output (image where lines are drawn on lanes)\n",
    "    result = image\n",
    "       \n",
    "    # Colour Selection\n",
    "    \n",
    "    # keep white and yellow pixels\n",
    "    RGB_thd_keep = [([150, 150, 150], [255, 255, 255]), ([160, 160, 100], [255, 210, 140])]\n",
    "    result, k                          = color_keep_range(result, RGB_thd_keep)\n",
    "    \n",
    "    # remove unwanted colours\n",
    "    RGB_thd_remove = [([0, 0, 0], [215, 215, 180])]\n",
    "    result, k                          = color_remove_range(result, RGB_thd_remove)\n",
    "    \n",
    "    # Gray scale\n",
    "    result = grayscale(result)\n",
    "    \n",
    "    # Gaussian filter\n",
    "    kernel_size     = 13\n",
    "    result = gaussian_blur(result, kernel_size)\n",
    "\n",
    "    # Canny edge detection\n",
    "    low_threshold   = 20\n",
    "    high_threshold  = 30\n",
    "    result = canny(result, low_threshold, high_threshold)\n",
    "    \n",
    "    # Zone of interest filtering\n",
    "    vertices        = np.array([[(0,np.int(image.shape[0]/2) + 100), \\\n",
    "                      (0,image.shape[0]-1), \\\n",
    "                      (image.shape[1]-1,image.shape[0]-1), \\\n",
    "                      (image.shape[1]-1 - 0,np.int(image.shape[0]/2) + 100), \\\n",
    "                      (np.int(image.shape[1]/2) + 100, np.int(image.shape[0]/2) + 50), \\\n",
    "                      (np.int(image.shape[1]/2) - 100, np.int(image.shape[0]/2) + 50)]], \\\n",
    "                    dtype=np.int32)\n",
    "    result = region_of_interest(result, vertices)\n",
    "    \n",
    "    # Hough Transform\n",
    "    minLineLength   = 1\n",
    "    maxLineGap      = 15\n",
    "    rho             = 1\n",
    "    theta           = np.pi/1440\n",
    "    minimum_vote    = 30\n",
    "    result          = hough_lines(result, rho, theta, minimum_vote, minLineLength, maxLineGap)\n",
    "    \n",
    "    # Display the result on original video\n",
    "    result          = weighted_img(result, image)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Preprocessed-video Credit : Udacity')\n",
    "#Load video\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip1_new        = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "\n",
    "#Process loaded video\n",
    "white_clip_new   = clip1_new.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "\n",
    "#Write output video\n",
    "white_output_new = 'test_videos_output/solidWhiteRight_out_new.mp4'\n",
    "%time white_clip_new.write_videofile(white_output_new, audio=False)\n",
    "\n",
    "#Display processed video\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessed-video Credit : Udacity')\n",
    "#Load video\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip2_new        = VideoFileClip(\"test_videos/solidYellowLeft.mp4\")\n",
    "\n",
    "#Process loaded video\n",
    "yellow_clip_new   = clip2_new.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "\n",
    "#Write output video\n",
    "yellow_output_new = 'test_videos_output/solidYellowLeft_out_new.mp4'\n",
    "%time yellow_clip_new.write_videofile(yellow_output_new, audio=False)\n",
    "\n",
    "#Display processed video\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is much better, but we still have some false-positives. We will find more sophisticated method in my next tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now, let's take a challenge video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Preprocessed-video Credit : Udacity')\n",
    "#Load video\n",
    "clip3        = VideoFileClip(\"test_videos/challenge.mp4\")\n",
    "\n",
    "#Process loaded video\n",
    "challenge_clip   = clip3.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "\n",
    "#Write output video\n",
    "challenge_output = 'test_videos_output/challenge_out.mp4'\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)\n",
    "\n",
    "#Display processed video\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our line detection algorithm with Canny Edge Detection and Hough Transform works badly on the challenge video with curve and many noises. we will need to find out a better method in my next tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "- [x] **Color Selection**  \n",
    "Our Color Selection is not perfect. A \"colour\" is perceived not only by R G B channels' absolute values but also by the ratio between them. We can easily enhance our function color_keep_range and color_remove_range by introducing ratio between R G B channels but it won't be intuitive and make thing more complicated. Thus, there is a very good alternative is to work with colour in HSL (Hue Saturation Lightness) or HSV (Hue Saturation Value).  \n",
    "HSL Scale, Image Credit : https://en.wikipedia.org/wiki/HSL_and_HSV#/media/File:HSL_color_solid_cylinder_saturation_gray.png &#8595;  \n",
    "<img src=\"images/640px-HSL_color_solid_cylinder_saturation_gray.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "We can precisely and intuitively remove unwanted colour in HSL or HSV scale.\n",
    "- [x] **Hough Transform vs Curve**  \n",
    "As we can see in challenge video, our algorithm does not work in curves, it can only detect straight lines. We can extend our algorithm with Generalized Hough Transform to fit our curves with a 3-rd degree polynomial.\n",
    "- [x] **Image distorsion**  \n",
    "Further from the image frame's center, more the image is distorted. So, the lines in the far left/right side is more distorted by camera's optical systems and made it more difficult to identify. We should need a distorsion corrector filter by applying our line detection algorithm.\n",
    "- [x] **Camera's perspective**  \n",
    "From bird-eye view, lines/curve are parallel, but in camera's perspective they converge. That makes it more difficult to deteect line in the far left/right side, they are contracted and displayed smaller than lines in image frame's center. So, we will also need a perspective transform to have a bird-eye view of the image frame.  \n",
    "Bird-eye view, Image Credit : M.Venkatesh, P.Vijayakumar, https://www.ijser.org/researchpaper/A-Simple-Birds-Eye-View-Transformation-Technique.pdf &#8595;  \n",
    "<img src=\"images/bird_eye_view.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "These enhancements will be used in my next tutorials of advanced lines detection technics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
